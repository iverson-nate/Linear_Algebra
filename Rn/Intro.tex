\section{Introduction}
\subsection{$\mathbb{R}^n$}
We will assume that all readers are already familiar with a concept of real and complex numbers. 
In this text we denote the real numbers by $\mathbb{R}$ and the complex numbers by $\mathbb{C}$. 

\begin{definition}\index{Rn@$\mathbb{R}^n$!Definition}
We define $\mathbb{R}^n$ to be the set of real column vectors in $n$-coordinates. That is 
\[
\mathbb{R}^n=\left\{ 
\begin{bmatrix}
x_1\\ x_2 \\ \vdots \\ x_n
\end{bmatrix} 
: x_i \in \mathbb{R}\text{ for all }i=1,2, \ldots, n \right\}.
\]
We will refer to the members of $\mathbb{R}^n$ as \emph{vectors} and the 
elements of $\mathbb{R}$ as \emph{scalars}. When we want to refer to row 
vectors instead of column vectors we will use the notation 
$\mathbb{R}^n_\text{row}$. The vector consisting of all zero coordinates is 
called the \emph{zero vector} and denoted $\vec{0}$.
\end{definition}

For convenience we may refer to each $\vec{x} \in \mathbb{R}^n$ as a single 
bold letter. We will for allow each coordinate to be labeled as the same 
letter subscriped with the coordinate number. For example if $\vec{x}, \vec{y}, \vec{z}, \vec{0} \in \mathbb{R}^n$
\[
\vec{x}=\begin{bmatrix}x_1\\ x_2 \\ \vdots \\ x_n\end{bmatrix}, \quad 
\vec{y}=\begin{bmatrix}y_1\\ y_2 \\ \vdots \\ y_n\end{bmatrix}, \quad 
\vec{v}=\begin{bmatrix}v_1\\ v_2 \\ \vdots \\ v_n\end{bmatrix}, \quad 
\vec{0}=\begin{bmatrix}0\\ 0 \\ \vdots \\ 0\end{bmatrix}.
\]

\begin{remark}
Sometimes it is useful to describe a vector as a function on it's coordinates. 
For example, if 
$\vec{v}=\begin{bmatrix}1 \\ 4 \\ 9 \end{bmatrix}$ We may say 
$\vec{v}(2)=4$ that is the \nth{2} coordinate of $\vec{v}$ is $4$.
\end{remark}


\begin{definition}[Vector Equality]\index{Rn@$\mathbb{R}^n$!Vector equality}
Vectors $\vec{v}, \vec{w} \in \mathbb{R}^n$ are \emph{equal},  if they are 
equal coordinatewise. That is, $v_i=v(i)=w(i)=w_i$ for all $i=1,2,\ldots, n$. 
We denote this with  $\vec{v}=\vec{w}$.
\end{definition}


\begin{definition}[Vector Addition]\index{Rn@$\mathbb{R}^n$!Vector addition}
We define \emph{vector addition} on $\mathbb{R}^n$ coordinatewise. 
That is, for $\vec{v},\vec{w} \in \mathbb{R}^n$ we define
\[
\vec{v}+\vec{w}=
\begin{bmatrix}v_1\\ v_2 \\ \vdots \\ v_n\end{bmatrix}+
\begin{bmatrix}w_1\\ w_2 \\ \vdots \\ w_n\end{bmatrix}=
\begin{bmatrix}v_1+w_1\\ v_2+w_2 \\ \vdots \\ v_n+w_n\end{bmatrix}.
\]
\end{definition}

\begin{definition}[Scalar Multiplication]\index{Rn@$\mathbb{R}^n$!Scalar multiplication}
We define \emph{scalar multiplication} on $\mathbb{R}^n$ coordinatewise. That 
is, for $\vec{v} \in \mathbb{R}^n$ and $r \in \mathbb{R}$ we define
\[
r\vec{v}=
r\begin{bmatrix}v_1\\ v_2 \\ \vdots \\ v_n\end{bmatrix}=
\begin{bmatrix}rv_1\\ rv_2 \\ \vdots \\ rv_n\end{bmatrix}.
\]
\end{definition}

The order of operations is scalar multiplication first then vector addition.

\begin{example}
Consider $\vec{x}=\begin{bmatrix*}[C]1\\ 0 \\ -3\end{bmatrix*}$ and
$\vec{y}=\begin{bmatrix*}[C]-4\\ 2 \\ 1\end{bmatrix*}$ in $\mathbb{R}^3$.
\[
2\vec{x}+(-1)\vec{y}=
2\begin{bmatrix*}[C]1\\ 0 \\ -3\end{bmatrix*}+
(-1)\begin{bmatrix*}[C]-4\\ 2 \\ 1\end{bmatrix*}=
%\begin{bmatrix}2(1)\\ 2(0) \\ 2(-3)\end{bmatrix}+
%\begin{bmatrix}(-1)(-4)\\ (-1)(2) \\ (-1)(1)\end{bmatrix}=
\begin{bmatrix*}[C]2\\ 0 \\ -6\end{bmatrix*}+
\begin{bmatrix*}[C]4\\ -2 \\ -1\end{bmatrix*}=
\begin{bmatrix}2+4\\ 0+(-2) \\ -6+(-1)\end{bmatrix}=
\begin{bmatrix*}[C]6\\ -2 \\ -7\end{bmatrix*}.
\]
This is actually also an example of our next definition.
\end{example}

\begin{definition}\index{Linear combination}
A \emph{linear combination} of vectors $\vec{a}_1, \vec{a}_2, \ldots,
\vec{a}_n$ is the sum
\[c_1\vec{a}_1+c_2\vec{a}_2+\cdots+c_n\vec{a}_n\]
for some choice of scalar multiples $c_1,c_2, \ldots, c_n \in \mathbb{R}$.
\end{definition}


\begin{example}
Any vector in $\mathbb{R}^2$ can be written as a linear combination of
$\begin{bmatrix}1 \\ 0\end{bmatrix}$ and $\begin{bmatrix}1 \\ 1 \end{bmatrix}$
because
\[
\begin{bmatrix}v_1 \\ v_2\end{bmatrix}=
(v_1-v_2) \begin{bmatrix}1 \\ 0\end{bmatrix}+
v_2 \begin{bmatrix}1 \\ 1 \end{bmatrix}.
\]
\end{example}

\begin{example}
The vector $\begin{bmatrix}1 \\ 1\end{bmatrix}$ is not a linear combination
of $\begin{bmatrix}1 \\ 0\end{bmatrix}$ and $\begin{bmatrix}2 \\
0\end{bmatrix}$. If it were then, we would have
\[
\begin{bmatrix}1 \\ 1\end{bmatrix}=
a \begin{bmatrix}1 \\ 0\end{bmatrix}+
b \begin{bmatrix}2 \\ 0 \end{bmatrix}
\]
for some constants $a,b\in \mathbb{R}$. Considering the second components, this
implies that $1=0$ which is a contradiction.
\end{example}

\begin{definition}
\index{Rn@$\mathbb{R}^n$!Standard basis}
The \emph{standard basis for} $\mathbb{R}^n$ is a set of $n$-vectors
$\mathcal{E}=\{\vec{e}_1, \vec{e}_2, \ldots, \vec{e}_n\}$ such that\\
$\vec{e}_j(k)=\begin{cases}
1 & \text{ if } k=j\\
0 & \text{ if } k\neq j
\end{cases}$
for each $j,k$ with $1\le j \le n$ and $1 \le k \le n$.
\end{definition}


\begin{remark}For each integer $n\geq 1$ the standard basis $\mathcal{E}$ 
of $\mathbb{R}^n$ can also be written as $n$ vectors:
\[
\vec{e}_{1}=\begin{bmatrix} 1 \\  0 \\ 0 \\ \vdots \\ \vdots \\ 0 \\ 0\end{bmatrix},
\vec{e}_{2}=\begin{bmatrix} 0 \\  1 \\ 0 \\ \vdots \\ \vdots \\ 0 \\ 0\end{bmatrix},
\vec{e}_{3}=\begin{bmatrix} 0 \\  0 \\ 1 \\ \vdots \\ \vdots \\ 0 \\ 0\end{bmatrix},
\cdots,
\vec{e}_k=\begin{bmatrix} 0 \\  \vdots \\ 0 \\ 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix}
\begin{array}{l}
\phantom{0} \\  \phantom{\vdots} \\ \phantom{0} \\ \gets k^{\tiny{\text{th}}} 
\text{ coordinate, } \\ \phantom{0} \\ \phantom{\vdots} \\ \phantom{0} 
\end{array} 
\cdots,
\vec{e}_{n-1}=\begin{bmatrix} 0 \\  0 \\ 0 \\ \vdots \\ \vdots \\ 1 \\ 0\end{bmatrix},
\vec{e}_{n}=\begin{bmatrix} 0 \\  0 \\ 0 \\ \vdots\\ \vdots \\ 0 \\ 
1\end{bmatrix}.
\]
\end{remark}
\begin{example}
In $\mathbb{R}^3 $ every vector is a linear combination of the standard basis 
$\mathcal{E}=\{\vec{e}_1,\vec{e}_2,\vec{e}_3\}$
\[
\begin{bmatrix}v_1 \\ v_2 \\ v_3\end{bmatrix}=
v_1 \begin{bmatrix}1 \\ 0 \\ 0\end{bmatrix}+
v_2 \begin{bmatrix}0 \\ 1 \\ 0\end{bmatrix}+
v_3 \begin{bmatrix}0 \\ 0 \\ 1\end{bmatrix}
=v_1\vec{e}_1+v_2\vec{e}_2+v_3\vec{e}_3.
\]
\end{example}

\begin{proposition}\label{prop:e_k_spans_Rn}
In $\mathbb{R}^n$ every vector is a linear combination of the standard basis 
$\mathcal{E}=\{\vec{e}_1, \vec{e}_2, \ldots, \vec{e}_n\}$.
\end{proposition}

\begin{proof}
Let $\vec{v} \in \mathbb{R}^n$ and $1\leq k\leq n$. Then we observe that  
\[\vec{v}(k)=v_k=v_k\cdot 1=v_k \vec{e}_k(k).\]
We can rewrite this equation by adding $k-1$ zeros before and $n-k$ zeros 
after $\vec{v}(k)$. That is,  
\[\vec{v}(k)=0+ \cdots + 0+v_k\vec{e}_k(k)+0+\cdots+0.\] 
Since $\vec{e}_j(k)=0$ for all $j\neq k$ we have 
\[\vec{v}(k)=v_1\vec{e}_1(k)+v_2\vec{e}_2(k)+\cdots+ v_k\vec{e}_k(k)+ \cdots + 
v_n\vec{e}_n(k).\]
Since the above equation is true for any $1\leq k\leq n$ we have,  
\[\vec{v}=v_1\vec{e}_1+v_2\vec{e}_2+\cdots+ v_n\vec{e}_n.\]
\end{proof}

\input{Rn/Rn_exercises.tex}

\subsection{Real Matrices}

\begin{definition}[Matrix]\index{Matrix!Definition}
A real matrix $A$ is a rectangular array of real numbers 
$a_{i,j} \in \mathbb{R}$ (usually the comma is omitted $a_{ij}$)  where the $i$ 
is the row and $j$ is the column number. That is, 
\[
A=[a_{ij}]=\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} 
\end{bmatrix}.
\]
The \emph{dimension} of the matrix is $m\times n$ where $m$ is the number of 
rows and $n$ is the number of columns. The set of all such matrices is denoted 
by $M_{m\times n}(\mathbb{R})$.\index{Matrix!Mmn@ $M_{m\times n}(\mathbb{R})$}
\end{definition}
The matrix $A$ can be also seen as a collection of vectors. Namely, 
$A=[a_{ij}]=[\vec{a}_1, \vec{a}_2, \ldots, \vec{a}_n]$ where 
 $\vec{a}_1, \vec{a}_2,\ldots, \vec{a}_n \in \mathbb{R}^n$ are the \emph{column 
vectors} of $A$. In particular, 
\[
\vec{a}_1=\begin{bmatrix}a_{11}\\a_{21}\\ \vdots \\ a_{m1}\end{bmatrix}, \quad 
\vec{a}_2=\begin{bmatrix}a_{12}\\a_{22}\\ \vdots \\ a_{m2}\end{bmatrix}, 
\cdots, \quad 
\vec{a}_k=\begin{bmatrix}a_{1k}\\a_{2k}\\ \vdots \\ a_{mk}\end{bmatrix}, 
\cdots, \quad 
\vec{a}_n=\begin{bmatrix}a_{1n}\\a_{2n}\\ \vdots \\ a_{mn}\end{bmatrix}.
\]

\begin{example}
$A=\begin{bmatrix*}[C]
2 & -3 & 0 \\
-5 & 0 & 1 
\end{bmatrix*}$
is a $2 \times 3$ matrix, $a_{12}=-3$, and  
$\vec{a}_1=\begin{bmatrix*}[C] 2 \\ -5 \end{bmatrix*}$.
\end{example}
We can also define Addition and scalar multiplication for matrices.

\begin{definition}[Matrix Addition] We define \emph{matrix addition} in each 
$M_{m\times n}(\mathbb{R})$ coordinatewise. That is for $A,B \in M_{m\times n}$
with $A=[a_{ij}]$ and $B=[b_{ij}]$ we define 
$A+B=[a_{ij}]+[b_{ij}]=[a_{ij}+b_{ij}]$ which is the same as saying:
\[
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{bmatrix}
+
\begin{bmatrix}
b_{11} & b_{12} & \cdots & b_{1n} \\
b_{21} & b_{22} & \cdots & b_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
b_{m1} & b_{m2} & \cdots & b_{mn}
\end{bmatrix}
=
\begin{bmatrix}
a_{11}+b_{11} & a_{12}+b_{12} & \cdots & a_{1n}+b_{1n} \\
a_{21}+b_{21} & a_{22}+b_{22} & \cdots & a_{2n}+b_{2n} \\
\vdots        & \vdots        & \ddots & \vdots \\
a_{m1}+b_{m1} & a_{m2}+b_{m2} & \cdots & a_{mn}+b_{mn}
\end{bmatrix}.
\]
\end{definition}

\begin{example}
\begin{align*}
\begin{bmatrix*}[C]
2  & -4 & 3 \\
-1 & 0  & 2
\end{bmatrix*}+
\begin{bmatrix*}[C]
1  & 3 & 2 \\
0 & -2  & -7
\end{bmatrix*}
&=\begin{bmatrix*}[C]
2+1  & -4+3 & 3+2 \\
-1+0 & 0-2  & 2-7
\end{bmatrix*}\\
&=\begin{bmatrix*}[C]
3  & -1 & 5 \\
-1 & -2  & -5
\end{bmatrix*}
\end{align*}
\end{example}

\begin{definition}[Matrix Scalar Multiplication] We define 
\emph{scalar multiplication} in each $M_{m\times n}$ coordinatewise. That is, 
if $A \in M_{m \times n}$ with $A=[a_{ij}]$ and $r \in \mathbb{R}$ we define 
$rA=r[a_{ij}]=[ra_{ij}]$ which is the same as saying:
\[
r\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{bmatrix}
=
\begin{bmatrix}
ra_{11} & ra_{12} & \cdots & ra_{1n} \\
ra_{21} & ra_{22} & \cdots & ra_{2n} \\
\vdots  & \vdots  & \ddots & \vdots \\
ra_{m1} & ra_{m2} & \cdots & ra_{mn}
\end{bmatrix}.
\]
\end{definition}

\begin{remark}
It is worth noting here that it is not accidental that both matrices and 
vectors have these two properties. For each $\vec{v} \in \mathbb{R}^n$ you can
think of it as an $n\times 1$ matrix $\vec{v} \in M_{n\times 1}(\mathbb{R})$. 
So all vectors are matrices. In Chapter~\ref{ch:Vector_Space} we will explore
a class called \emph{Vector Spaces} that share properties with matrices and 
vectors.
\end{remark}

The order of operations is again scalar multiplication then addition.

\begin{example}
\begin{align*}
2\begin{bmatrix*}[C]
2  & -4 & 3 \\
-1 & 0  & 2
\end{bmatrix*}+
(-3)\begin{bmatrix*}[C]
1  & 3 & 2 \\
0 & -2  & -7
\end{bmatrix*}
&=
\begin{bmatrix*}[C]
4  & -8 & 6 \\
-2 & 0  & 4
\end{bmatrix*}+
\begin{bmatrix*}[C]
-3 & -9  & -6 \\
0  & 6 & 21
\end{bmatrix*}\\
&=\begin{bmatrix*}[C]
4-3  & -8-9 & 6-6 \\
-2+0 & 0+6  & 4+21
\end{bmatrix*}\\
&=\begin{bmatrix*}[C]
1  & -17 & 0 \\
-2 & 6  & 25
\end{bmatrix*}
\end{align*}
\end{example}
\input{Rn/Real_Matrices__exercises.tex}

\subsection{Matrix-Vector Multiplication}

\begin{definition}[Matrix-Vector Multiplication]
\index{Matrix!Matrix-Vector Multiplication}
Let $A=[a_{ij}]\in M_{m\times n}(\mathbb{R})$ and $\vec{v} \in \mathbb{R}^n$.  
We define the \emph{Matrix-Vector} product to be the linear combination 
\[
A\vec{v}=[\vec{a}_1, \vec{a}_2, \ldots ,\vec{a}_n]\begin{bmatrix}v_1 \\ v_2 \\ 
\vdots \\ v_n \end{bmatrix}=v_1\vec{a}_1+v_2\vec{a}_2+\cdots+v_n\vec{a}_n.
\]
\end{definition} 
\begin{remark}
It is important to note that the product is only defined if the number of columns in $A$ matches the number of coordinates in $\vec{v}$.
\end{remark}

\begin{example}\label{ex:mv_mult_by_def}
\begin{align*}
\begin{bmatrix*}[C]
2 & -3 & 0 \\
-5 & 0 & 1 
\end{bmatrix*} 
\begin{bmatrix}1\\ 2 \\ 3 \end{bmatrix}
&=1\begin{bmatrix*}[C]2\\-5\end{bmatrix*} 
+2\begin{bmatrix*}[C]-3\\0\end{bmatrix*} 
+3\begin{bmatrix}0\\1\end{bmatrix}\\
&=\begin{bmatrix*}[C]2\\-5\end{bmatrix*}
+\begin{bmatrix*}[C]-6\\0\end{bmatrix*}+\begin{bmatrix}0\\3\end{bmatrix}\\
&=\begin{bmatrix*}[C]-4\\-2\end{bmatrix*}
\end{align*}
\end{example}

\begin{example}\label{ex:mv_mult_by_def_gen}
\begin{align*}
\begin{bmatrix*}[C]
2 & -3 & 0 \\
-5 & 0 & 1 
\end{bmatrix*}
\begin{bmatrix}v_1\\ v_2 \\ v_3 \end{bmatrix}
&=v_1\begin{bmatrix*}[C]2\\-5 \end{bmatrix*}
+v_2\begin{bmatrix*}[C]-3 \\0\end{bmatrix*}
+v_3\begin{bmatrix}0\\1\end{bmatrix}\\
&=\begin{bmatrix*}[C]2v_1\\-5v_1\end{bmatrix*}
+\begin{bmatrix}-3v_2\\ 0\end{bmatrix} 
+\begin{bmatrix}0\\v_3\end{bmatrix}\\
&=\begin{bmatrix}2v_1-3v_2\\-5v_1+v_3\end{bmatrix}\\
&=\begin{bmatrix*}[C]2v_1-3v_2+0v_3\\-5v_1+0v_2+v_3\end{bmatrix*}
\end{align*}
Notice that we put some zero values back in on the final result. This is not 
strictly necessary but helps motivate the row-column algorithm for matrix 
multiplication.
\end{example}

\begin{proposition}[The Row-Column Algorithm]\label{prop:vec_row-column}
\index{Matrix!Matrix-Vector Multiplication!The Row-Column Algorithm}
Let $A=[a_{ij}]\in M_{m\times n}(\mathbb{R})$ and $\vec{v} \in \mathbb{R}^n$. 
Then $\vec{w}=A\vec{v} \in \mathbb{R}^m$ where  
\[\vec{w}(k)=a_{k1}v_1+a_{k2}v_2+\cdots+a_{kn}v_n\] 
for all $k$ with $1 \le k \le n$.
\end{proposition}
\begin{remark} 
This can be visualized by taking each row in $A$ and multiplying 
each of it's entries by the corresponding coordinate in $\vec{v}$. The sum of 
these entries is the same row in the result as the row in $A$.\\
\input{Rn/vector_row-product_tikz_illustration.tex}
\end{remark}
\begin{proof}
Suppose $A=[a_{ij}]=[\vec{a}_1,\vec{a}_2,\ldots,\vec{a}_n]$ and  
$\vec{v} \in \mathbb{R}^n$. Then 
\[
w=A\vec{v}=[\vec{a}_1,\vec{a}_2,\ldots,\vec{a}_n] \begin{bmatrix}v_1\\ 
v_2 \\ 
\vdots \\ 
v_n\end{bmatrix}=v_1\vec{a}_1+v_2\vec{a}_2+\cdots+v_n\vec{a}_n.
\]
Let $k$ be an integer with $1 \le k \le n$. Since 
$w=A\vec{v}=v_1\vec{a}_1+v_2\vec{a}_2+\cdots+v_n\vec{a}_n$ as vectors they are 
also equal at the $k^{\tiny\text{th}}$ coordinate. That is, 
$w(k)=v_1\vec{a}_1(k)+v_2\vec{a}_2(k)+\cdots+v_n\vec{a}_n(k)$. 
Note that the column vector 
$\vec{a}_j=\begin{bmatrix}a_{1j}\\ a_{2j}\\ \vdots \\ a_{nj}\end{bmatrix}$ for 
each $j$. That is, $\vec{a}_j(k)=a_{kj}$. Therefore, 
\[\vec{w}(k)=v_1a_{k1}+v_2a_{k2}+\cdots+v_na_{kn}=a_{k1}v_1+a_{k2}v_2+\cdots+a_{
kn}v_n.\]
\end{proof}

\begin{example}
For $A=\begin{bmatrix*}[C]
1  & 1 \\
0  & 1 \\
3  & -4\\
-1 & 0 
\end{bmatrix*}$\\
\begin{inparaenum}[a.)]
\item Is there a solution to 
$A\vec{v}=\begin{bmatrix*}[C] 0 \\ -2 \\ 14 \\ -2 \end{bmatrix*}$ ? 
Picking a generic  $\vec{v}\in\mathbb{R}^n$ we can multiply
\[
A\vec{v}=
\begin{bmatrix*}[C]
1  & 1 \\
0  & 1 \\
3  & -4\\
-1 & 0 
\end{bmatrix*}\begin{bmatrix}v_1\\v_2\end{bmatrix}
=\begin{bmatrix*}
v_1+v_2\\
v_2\\
3v_1-4v_2\\
-v_1
\end{bmatrix*}
=\begin{bmatrix*}[C] 0 \\ -2 \\ 14 \\ -2 \end{bmatrix*}.
\]
By inspection we can see that $v_2=-2$ and $-v_1=-2$ so $v_1=2$. We check the
rest of the values by multiplying
\[
\begin{bmatrix*}[C]
1  & 1 \\
0  & 1 \\
3  & -4\\
-1 & 0 
\end{bmatrix*}\begin{bmatrix*}[C]2\\-2\end{bmatrix*}
=\begin{bmatrix*}[C] 0 \\ -2 \\ 14 \\ -2 \end{bmatrix*}.
\]
So we can conclude there is a solution, namely 
$\vec{v}=\begin{bmatrix*}[C]2\\-2\end{bmatrix*}$.\\
%
\item Is there a solution, $\vec{v} \in \mathbb{R}^2$, to the 
matrix equation $A\vec{v}=\vec{b}$ 
for every $\vec{b} \in \mathbb{R}^4$?

Recall for a generic $\vec{v} \in \mathbb{R}^n$
\[A\vec{v}=\begin{bmatrix*}[C]v_1+v_2\\v_2\\3v_1-4v_2\\-v_1\end{bmatrix*}\]
So if $A\vec{v}=\vec{b}$ has a solution for 
$\vec{b}=\begin{bmatrix*}b_1\\0\\b_3\\0\end{bmatrix*}$, the values $v_1=v_2=0$ 
which would mean $b_1=v_1+v_2=0$ and $b_3=3v_1-4v_2=0$. So if we pick
$b=\begin{bmatrix}1 \\ 0 \\ 0 \\ 0\end{bmatrix}$ this $A\vec{v}=\vec{b}$ 
cannot have a solution.
\end{inparaenum}
\end{example}

\input{Rn/Matrix-Vector_Multiplication_exercises.tex}
